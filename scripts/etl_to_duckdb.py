#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ® ETL (æå–ã€è½¬æ¢ã€åŠ è½½) è„šæœ¬

åŠŸèƒ½:
1. ä»æŒ‡å®šç›®å½•æå– Excel (.xlsx, .xls) å’Œ .csv æ–‡ä»¶ã€‚
2. åº”ç”¨ä¸šåŠ¡é€»è¾‘è¿›è¡Œæ•°æ®è½¬æ¢ã€æ¸…æ´—ã€è®¡ç®—ï¼Œå†…ç½®â€œåˆ«å-éªŒè¯â€æ¨¡å¼ã€‚
3. å°†å¤„ç†åçš„å¹²å‡€æ•°æ®åŠ è½½åˆ°å•ä¸ª DuckDB æ•°æ®åº“æ–‡ä»¶ä¸­ã€‚
4. åˆ›å»ºç´¢å¼•å¹¶ä¼˜åŒ–æ•°æ®åº“ä»¥æé«˜æŸ¥è¯¢æ€§èƒ½ã€‚

ä½¿ç”¨æ–¹æ³•:
    # ä½¿ç”¨é»˜è®¤è·¯å¾„ (è¾“å…¥: 'å®é™…æ•°æ®/', è¾“å‡º: 'insurance_data.duckdb')
    python scripts/etl_to_duckdb.py

    # æŒ‡å®šè¾“å…¥å’Œè¾“å‡ºè·¯å¾„
    python scripts/etl_to_duckdb.py --input-dir /path/to/data --output-db /path/to/output.duckdb
"""

import os
import re
import sys
import glob
import argparse
import duckdb
import pandas as pd
from datetime import datetime
from pathlib import Path

class DataProcessor:
    """
    æ•°æ®å¤„ç†å™¨ - æ ¸å¿ƒé€»è¾‘ä» app.py ç§»æ¤è€Œæ¥ã€‚
    è´Ÿè´£æ‰€æœ‰çš„æ•°æ®è½¬æ¢ã€éªŒè¯å’Œè®¡ç®—ã€‚
    """
    
    def __init__(self):
        # æœ€ç»ˆè¾“å‡ºçš„27ä¸ªæ ‡å‡†å­—æ®µ
        self.required_fields = [
            'snapshot_date', 'policy_start_year', 'business_type_category', 'chengdu_branch',
            'second_level_organization', 'third_level_organization', 'customer_category_3',
            'insurance_type', 'is_new_energy_vehicle', 'coverage_type', 'is_transferred_vehicle',
            'renewal_status', 'vehicle_insurance_grade', 'highway_risk_grade', 'large_truck_score',
            'small_truck_score', 'terminal_source', 'signed_premium_yuan', 'matured_premium_yuan',
            'policy_count', 'claim_case_count', 'reported_claim_payment_yuan', 'expense_amount_yuan',
            'commercial_premium_before_discount_yuan', 'premium_plan_yuan',
            'marginal_contribution_amount_yuan', 'week_number'
        ]
        
        # å­—æ®µåˆ«åæ˜ å°„ï¼Œç”¨äºå…¼å®¹ä¸åŒè¯­è¨€ç¯å¢ƒçš„åˆ—å
        self.field_alias_mapping = {
            'snapshot_date': ['snapshot_date', 'åˆ·æ–°æ—¶é—´', 'Snapshot Date'],
            'policy_start_year': ['policy_start_year', 'ä¿é™©èµ·æœŸ', 'Policy Start Year'],
            'business_type_category': ['business_type_category', 'ä¸šåŠ¡ç±»å‹åˆ†ç±»', 'Business Type Category'],
            'chengdu_branch': ['chengdu_branch', 'æˆéƒ½ä¸­æ”¯', 'Chengdu Branch'],
            'second_level_organization': ['second_level_organization', 'äºŒçº§æœºæ„', 'Second Level Organization'],
            'third_level_organization': ['third_level_organization', 'ä¸‰çº§æœºæ„', 'Third Level Organization'],
            'customer_category_3': ['customer_category_3', 'å®¢æˆ·ç±»åˆ«3', 'Customer Category 3'],
            'insurance_type': ['insurance_type', 'é™©ç§ç±»', 'Insurance Type'],
            'is_new_energy_vehicle': ['is_new_energy_vehicle', 'æ˜¯å¦æ–°èƒ½æºè½¦1', 'Is New Energy Vehicle'],
            'coverage_type': ['coverage_type', 'äº¤ä¸‰/ä¸»å…¨', 'Coverage Type'],
            'is_transferred_vehicle': ['is_transferred_vehicle', 'æ˜¯å¦è¿‡æˆ·è½¦', 'Is Transferred Vehicle'],
            'renewal_status': ['renewal_status', 'ç»­ä¿æƒ…å†µ', 'Renewal Status'],
            'vehicle_insurance_grade': ['vehicle_insurance_grade', 'è½¦é™©åˆ†ç­‰çº§', 'Vehicle Insurance Grade'],
            'highway_risk_grade': ['highway_risk_grade', 'é«˜é€Ÿé£é™©ç­‰çº§', 'Highway Risk Grade'],
            'large_truck_score': ['large_truck_score', 'å¤§è´§è½¦è¯„åˆ†', 'Large Truck Score'],
            'small_truck_score': ['small_truck_score', 'å°è´§è½¦è¯„åˆ†', 'Small Truck Score'],
            'terminal_source': ['terminal_source', 'ç»ˆç«¯æ¥æº', 'Terminal Source'],
            'signed_premium_wan': ['signed_premium_wan', 'è·Ÿå•ä¿è´¹(ä¸‡)', 'è·Ÿå•ä¿è´¹(Ten Thousand)'],
            'average_premium': ['average_premium', 'å•å‡ä¿è´¹', 'Average Premium'],
            'matured_premium_wan': ['matured_premium_wan', 'æ»¡æœŸå‡€ä¿è´¹(ä¸‡)', 'æ»¡æœŸå‡€ä¿è´¹(Ten Thousand)'],
            'claim_frequency': ['claim_frequency', 'å‡ºé™©é¢‘åº¦', 'Claim Frequency'],
            'claim_case_count': ['claim_case_count', 'æ¡ˆä»¶æ•°', 'Claim Case Count'],
            'average_claim_amount': ['average_claim_amount', 'æ¡ˆå‡èµ”æ¬¾', 'Average Claim Amount'],
            'total_claim_wan': ['total_claim_wan', 'æ€»èµ”æ¬¾(ä¸‡)', 'æ€»èµ”æ¬¾(Ten Thousand)'],
            'matured_claim_ratio': ['matured_claim_ratio', 'æ»¡æœŸèµ”ä»˜ç‡', 'Matured Claim Ratio'],
            'expense_ratio': ['expense_ratio', 'è´¹ç”¨ç‡', 'Expense Ratio'],
            'variable_cost_ratio': ['variable_cost_ratio', 'å˜åŠ¨æˆæœ¬ç‡', 'Variable Cost Ratio'],
            'commercial_autonomous_coefficient': ['commercial_autonomous_coefficient', 'å•†ä¸šé™©è‡ªä¸»ç³»æ•°', 'Commercial Autonomous Coefficient'],
            'week_number': ['week_number', 'å‘¨æ¬¡', 'Week Number']
        }
        
        self.boolean_map = {'æ˜¯': True, 'å¦': False, 'Y': True, 'N': False, 'true': True, 'false': False, True: True, False: False}

    def standardize_fields(self, df, original_filename=None, user_week_number=None):
        """æ ‡å‡†åŒ–å­—æ®µåå’Œæ•°æ®ç±»å‹, ä½¿ç”¨åˆ«åç³»ç»Ÿå…¼å®¹å¤šè¯­è¨€åˆ—å"""
        # é¢„å¤„ç†ï¼šå¦‚æœæ˜¯å·²å¤„ç†çš„æ•°æ®ï¼ˆåŒ…å«æœ€ç»ˆå­—æ®µï¼‰ï¼Œåˆ™é€†å‘ç”Ÿæˆå¿…è¦çš„ä¸­é—´å­—æ®µ
        if 'signed_premium_yuan' in df.columns:
            # Helper to safely convert to numeric
            def safe_numeric(col): return pd.to_numeric(df[col], errors='coerce').fillna(0)
            
            if 'signed_premium_wan' not in df.columns:
                df['signed_premium_wan'] = safe_numeric('signed_premium_yuan') / 10000
            
            if 'matured_premium_yuan' in df.columns and 'matured_premium_wan' not in df.columns:
                df['matured_premium_wan'] = safe_numeric('matured_premium_yuan') / 10000
                
            if 'reported_claim_payment_yuan' in df.columns and 'total_claim_wan' not in df.columns:
                df['total_claim_wan'] = safe_numeric('reported_claim_payment_yuan') / 10000
                
            if 'expense_amount_yuan' in df.columns and 'expense_ratio' not in df.columns:
                # expense_ratio = expense_amount / signed_premium
                # Handle division by zero
                sp = safe_numeric('signed_premium_yuan')
                ea = safe_numeric('expense_amount_yuan')
                df['expense_ratio'] = ea / sp.replace(0, 1) # Avoid div by zero, will be 0/1=0 if sp is 0 but ea is 0. If ea>0 sp=0, it's problematic but let's assume 0.
                df.loc[sp == 0, 'expense_ratio'] = 0

            if 'marginal_contribution_amount_yuan' in df.columns and 'variable_cost_ratio' not in df.columns:
                # variable_cost_ratio = 1 - (marginal_contribution / matured_premium)
                mp = safe_numeric('matured_premium_yuan')
                mc = safe_numeric('marginal_contribution_amount_yuan')
                df['variable_cost_ratio'] = 1 - (mc / mp.replace(0, 1))
                df.loc[mp == 0, 'variable_cost_ratio'] = 0
                
            if 'average_premium' not in df.columns and 'policy_count' in df.columns:
                 # average_premium = matured_premium / policy_count
                 mp = safe_numeric('matured_premium_yuan')
                 pc = safe_numeric('policy_count')
                 df['average_premium'] = mp / pc.replace(0, 1)
                 df.loc[pc == 0, 'average_premium'] = 0

            if 'commercial_premium_before_discount_yuan' in df.columns and 'commercial_autonomous_coefficient' not in df.columns:
                # coeff = matured_premium / commercial_premium_before_discount
                mp = safe_numeric('matured_premium_yuan')
                cp = safe_numeric('commercial_premium_before_discount_yuan')
                df['commercial_autonomous_coefficient'] = mp / cp.replace(0, 1)
                df.loc[cp == 0, 'commercial_autonomous_coefficient'] = 1.0

        rename_map = {}
        found_internal_fields = set()
        input_columns = df.columns.tolist()
        
        for internal_name, aliases in self.field_alias_mapping.items():
            for alias in aliases:
                if alias in input_columns:
                    rename_map[alias] = internal_name
                    found_internal_fields.add(internal_name)
                    break
        
        required_for_calc = {
            'signed_premium_wan', 'matured_premium_wan', 'average_premium', 
            'claim_case_count', 'total_claim_wan', 'expense_ratio', 
            'variable_cost_ratio', 'commercial_autonomous_coefficient'
        }
        
        missing_fields = [f"'{field}' (åˆ«å: {', '.join(self.field_alias_mapping.get(field, []))})" for field in required_for_calc if field not in found_internal_fields]

        if missing_fields:
            raise ValueError(f"å¤„ç†å¤±è´¥ï¼šè¾“å…¥æ–‡ä»¶ '{original_filename}' ç¼ºå°‘ä»¥ä¸‹å¿…éœ€çš„åˆ—ï¼š\n" + "\n".join(missing_fields))

        df_renamed = df.rename(columns=rename_map)
        result_df = df_renamed.copy()
        
        # ç¡®ä¿å…³é”®ç»´åº¦å­—æ®µå­˜åœ¨ï¼Œå¦åˆ™èµ‹äºˆé»˜è®¤å€¼
        for field in ['is_new_energy_vehicle', 'is_transferred_vehicle']:
            if field not in result_df.columns: result_df[field] = False
            result_df[field] = result_df[field].apply(lambda x: self.boolean_map.get(x, False))

        if 'policy_start_year' in result_df.columns:
            def extract_year(value):
                if pd.isna(value): return 0
                try:
                    if isinstance(value, str):
                        text = value.strip()
                        if not text: return 0
                        year_match = re.search(r'(19|20)\d{2}', text)
                        if year_match: return int(year_match.group(0))
                        date_obj = pd.to_datetime(text, errors='coerce')
                        return int(date_obj.year) if pd.notna(date_obj) else 0
                    if isinstance(value, (int, float)) and 1900 <= value <= 2100: return int(value)
                    date_obj = pd.to_datetime(value, errors='coerce')
                    return int(date_obj.year) if pd.notna(date_obj) else 0
                except Exception: return 0
            result_df['policy_start_year'] = result_df['policy_start_year'].apply(extract_year)
        else:
            result_df['policy_start_year'] = 0

        week_number = 40
        if user_week_number is not None:
            week_number = user_week_number
        elif original_filename:
            patterns = [r'ç¬¬(\d+)å‘¨', r'å‘¨(\d+)', r'W(\d+)', r'week\s*(\d+)', r'(\d+)å‘¨']
            for pattern in patterns:
                match = re.search(pattern, original_filename, re.IGNORECASE)
                if match:
                    week_number = int(match.group(1))
                    break
        result_df['week_number'] = week_number
        
        # ç¡®ä¿äºŒçº§æœºæ„å§‹ç»ˆä¸º'å››å·'
        result_df['second_level_organization'] = 'å››å·'
        
        # ä¸ºå…¶ä»–ç¼ºå¤±çš„ç»´åº¦å­—æ®µè®¾ç½®å®‰å…¨é»˜è®¤å€¼
        for field in ['snapshot_date', 'business_type_category', 'chengdu_branch', 'third_level_organization', 'customer_category_3', 'insurance_type', 'coverage_type', 'renewal_status', 'vehicle_insurance_grade', 'highway_risk_grade', 'large_truck_score', 'small_truck_score', 'terminal_source']:
            if field not in result_df.columns:
                result_df[field] = '' if field != 'snapshot_date' else pd.NaT

        return result_df

    def calculate_absolute_fields(self, df):
        """è®¡ç®—9ä¸ªç»å¯¹å€¼å­—æ®µ"""
        result_df = df.copy()
        
        # Helper to safely convert to numeric
        def to_numeric(series):
            return pd.to_numeric(series, errors='coerce').fillna(0)

        signed_premium_wan = to_numeric(df.get('signed_premium_wan', 0))
        matured_premium_wan = to_numeric(df.get('matured_premium_wan', 0))
        avg_premium = to_numeric(df.get('average_premium', 1.0)).replace(0, 1.0)
        coeff = to_numeric(df.get('commercial_autonomous_coefficient', 1.0)).replace(0, 1.0)
        expense_ratio = to_numeric(df.get('expense_ratio', 0))
        variable_cost_ratio = to_numeric(df.get('variable_cost_ratio', 0))

        result_df['signed_premium_yuan'] = signed_premium_wan * 10000
        result_df['matured_premium_yuan'] = matured_premium_wan * 10000
        result_df['commercial_premium_before_discount_yuan'] = result_df['matured_premium_yuan'] / coeff
        result_df['policy_count'] = (result_df['matured_premium_yuan'] / avg_premium).round().astype(int)
        result_df['claim_case_count'] = to_numeric(df.get('claim_case_count', 0)).astype(int)
        result_df['reported_claim_payment_yuan'] = to_numeric(df.get('total_claim_wan', 0)) * 10000
        result_df['expense_amount_yuan'] = result_df['signed_premium_yuan'] * expense_ratio
        result_df['premium_plan_yuan'] = result_df['matured_premium_yuan'] # é»˜è®¤ç­‰äºæ»¡æœŸä¿è´¹
        result_df['marginal_contribution_amount_yuan'] = result_df['matured_premium_yuan'] * (1 - variable_cost_ratio)
        
        return result_df

    def finalize_output(self, df):
        """ç¡®ä¿æœ€ç»ˆè¾“å‡ºçš„å­—æ®µã€é¡ºåºå’Œç±»å‹æ­£ç¡®"""
        output_df = pd.DataFrame()
        for field in self.required_fields:
            output_df[field] = df.get(field)
        
        # ç±»å‹è½¬æ¢
        for col in output_df.columns:
            if 'yuan' in col or 'amount' in col or 'score' in col:
                output_df[col] = pd.to_numeric(output_df[col], errors='coerce').fillna(0)
            elif 'count' in col:
                output_df[col] = pd.to_numeric(output_df[col], errors='coerce').fillna(0).astype(int)
            elif 'year' in col or 'week' in col:
                 output_df[col] = pd.to_numeric(output_df[col], errors='coerce').fillna(0).astype(int)
            elif 'date' in col:
                output_df[col] = pd.to_datetime(output_df[col], errors='coerce')
        return output_df

class ETLConverter:
    """ETL è½¬æ¢å™¨"""

    def __init__(self, input_dir: str, output_db: str, table_name: str):
        self.input_dir = input_dir
        self.output_db = output_db
        self.table_name = table_name
        self.conn = None
        self.data_processor = DataProcessor()

    def find_files(self) -> list:
        """æŸ¥æ‰¾æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶ (Excel, CSV)"""
        patterns = ["*.xlsx", "*.xls", "*.csv"]
        files = []
        for pattern in patterns:
            files.extend(glob.glob(os.path.join(self.input_dir, pattern)))
        
        if not files:
            print(f"âŒ é”™è¯¯: åœ¨ç›®å½• '{self.input_dir}' ä¸­æœªæ‰¾åˆ°ä»»ä½• .xlsx, .xls, æˆ– .csv æ–‡ä»¶")
            sys.exit(1)

        print(f"ğŸ“ æ‰¾åˆ° {len(files)} ä¸ªå¾…å¤„ç†æ–‡ä»¶:")
        total_size_mb = 0
        for i, file in enumerate(files, 1):
            size_mb = os.path.getsize(file) / (1024 * 1024)
            total_size_mb += size_mb
            print(f"   {i}. {Path(file).name} ({size_mb:.2f} MB)")
        print(f"   æ€»å¤§å°: {total_size_mb:.2f} MB")
        return files

    def create_database(self):
        """åˆ›å»ºæˆ–è¦†ç›–æ•°æ®åº“"""
        print(f"\nğŸ”¨ å‡†å¤‡æ•°æ®åº“: {self.output_db}")
        if os.path.exists(self.output_db):
            os.remove(self.output_db)
            print(f"   âš ï¸  å·²åˆ é™¤æ—§çš„æ•°æ®åº“æ–‡ä»¶")
        self.conn = duckdb.connect(self.output_db)
        print("   âœ… æ•°æ®åº“è¿æ¥å·²å»ºç«‹")

    def process_and_import_files(self, files: list):
        """å¤„ç†å¹¶å¯¼å…¥æ‰€æœ‰æ–‡ä»¶"""
        print(f"\nğŸ“¥ å¼€å§‹å¤„ç†å’Œå¯¼å…¥æ–‡ä»¶...")
        total_start = datetime.now()

        for i, file_path in enumerate(files, 1):
            file_start = datetime.now()
            filename = Path(file_path).name
            print(f"\n   [{i}/{len(files)}] å¤„ç†: {filename}")

            try:
                # 1. æå– (Extract)
                if file_path.endswith(('.xlsx', '.xls')):
                    df = pd.read_excel(file_path)
                else:
                    df = pd.read_csv(file_path)
                
                # 2. è½¬æ¢ (Transform)
                df_std = self.data_processor.standardize_fields(df, filename)
                df_calc = self.data_processor.calculate_absolute_fields(df_std)
                final_df = self.data_processor.finalize_output(df_calc)
                
                # 3. åŠ è½½ (Load)
                if i == 1:
                    self.conn.execute(f"CREATE TABLE {self.table_name} AS SELECT * FROM final_df")
                    print(f"      âœ… è¡¨ '{self.table_name}' å·²åˆ›å»º")
                else:
                    self.conn.execute(f"INSERT INTO {self.table_name} SELECT * FROM final_df")
                    print(f"      âœ… æ•°æ®å·²è¿½åŠ ")
                
                elapsed = (datetime.now() - file_start).total_seconds()
                print(f"      â±ï¸  è€—æ—¶: {elapsed:.2f}ç§’")

            except Exception as e:
                print(f"      âŒ å¤„ç†å¤±è´¥: {e}")
                raise

        total_elapsed = (datetime.now() - total_start).total_seconds()
        print(f"\nâœ… æ‰€æœ‰æ–‡ä»¶å¯¼å…¥å®Œæˆï¼Œæ€»è€—æ—¶: {total_elapsed:.2f}ç§’")

    def clean_data(self):
        """æ•°æ®æ¸…æ´—ï¼šåˆ é™¤å…³é”®æŒ‡æ ‡ä¸ºç©ºçš„è®°å½•"""
        print(f"\nğŸ§¹ æ•°æ®æ¸…æ´—...")
        result = self.conn.execute(f"""
            DELETE FROM {self.table_name}
            WHERE policy_start_year = 0 OR signed_premium_yuan = 0 OR week_number = 0
        """)
        deleted_count = result.fetchall()[0][0] if result else 0
        if deleted_count > 0: print(f"   âš ï¸  åˆ é™¤äº† {deleted_count} æ¡æ— æ•ˆè®°å½•")
        else: print(f"   âœ… æ•°æ®å®Œæ•´ï¼Œæ— éœ€æ¸…ç†")

    def create_indexes(self):
        """åˆ›å»ºç´¢å¼•ä»¥ä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½"""
        print(f"\nâš¡ åˆ›å»ºç´¢å¼•...")
        indexes = [
            ("idx_week", "week_number"), ("idx_year", "policy_start_year"),
            ("idx_org", "third_level_organization"), ("idx_business", "business_type_category"),
            ("idx_year_week", "policy_start_year, week_number"),
        ]
        for idx_name, columns in indexes:
            try:
                self.conn.execute(f"CREATE INDEX {idx_name} ON {self.table_name}({columns})")
                print(f"   âœ… {idx_name} on ({columns})")
            except Exception as e:
                print(f"   âš ï¸  {idx_name} åˆ›å»ºå¤±è´¥: {e}")

    def analyze_data(self):
        """æ•°æ®åˆ†æå¹¶æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯"""
        print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
        stats_q = f"""
            SELECT COUNT(*), MIN(policy_start_year), MAX(policy_start_year),
                   COUNT(DISTINCT week_number), MIN(week_number), MAX(week_number),
                   COUNT(DISTINCT third_level_organization), SUM(signed_premium_yuan), SUM(policy_count)
            FROM {self.table_name}
        """
        stats = self.conn.execute(stats_q).fetchone()
        print(f"""
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
   æ€»è®°å½•æ•°:     {stats[0]:,} æ¡
   å¹´ä»½èŒƒå›´:     {stats[1]} ~ {stats[2]}
   å‘¨æ¬¡èŒƒå›´:     ç¬¬ {stats[4]} å‘¨ ~ ç¬¬ {stats[5]} å‘¨ (å…± {stats[3]} å‘¨)
   ä¸‰çº§æœºæ„:     {stats[6]} ä¸ª
   ç­¾å•ä¿è´¹:     {stats[7]/10000:,.2f} ä¸‡å…ƒ
   ä¿å•ä»¶æ•°:     {stats[8]:,} ä»¶
   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        """)

    def optimize_database(self):
        """ä¼˜åŒ–æ•°æ®åº“"""
        print(f"\nğŸ—œï¸  ä¼˜åŒ–æ•°æ®åº“...")
        self.conn.execute("VACUUM; ANALYZE;")
        print(f"   âœ… æ•°æ®åº“å·²ä¼˜åŒ–")

    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.conn:
            self.conn.close()
            print(f"\nâœ… æ•°æ®åº“è¿æ¥å·²å…³é—­")

    def run(self):
        """æ‰§è¡Œå®Œæ•´çš„ETLæµç¨‹"""
        try:
            print("=" * 80)
            print("ğŸš€ æ•°æ® ETL (Excel/CSV -> DuckDB) è½¬æ¢å·¥å…·")
            print("=" * 80)

            files = self.find_files()
            self.create_database()
            self.process_and_import_files(files)
            self.clean_data()
            self.create_indexes()
            self.optimize_database()
            self.analyze_data()

            print("\n" + "=" * 80)
            print("ğŸ‰ ETL æµç¨‹æˆåŠŸå®Œæˆï¼")
            print(f"ä¸‹ä¸€æ­¥: åœ¨æ‚¨çš„åˆ†æå·¥å…·æˆ–åº”ç”¨ä¸­è¿æ¥ {self.output_db} æ–‡ä»¶ã€‚")
            print("=" * 80)

        except Exception as e:
            print(f"\nâŒ è½¬æ¢å¤±è´¥: {e}", file=sys.stderr)
            import traceback
            traceback.print_exc()
            sys.exit(1)
        finally:
            self.close()

def main():
    """ä¸»å‡½æ•°ï¼šè§£æå‚æ•°å¹¶å¯åŠ¨è½¬æ¢å™¨"""
    parser = argparse.ArgumentParser(description="ä» Excel/CSV æ–‡ä»¶åˆ° DuckDB çš„ ETL å·¥å…·ã€‚")
    parser.add_argument(
        "--input-dir",
        type=str,
        default="å®é™…æ•°æ®",
        help="åŒ…å«æºæ•°æ®æ–‡ä»¶ (Excel/CSV) çš„ç›®å½•è·¯å¾„ã€‚"
    )
    parser.add_argument(
        "--output-db",
        type=str,
        default="insurance_data.duckdb",
        help="è¾“å‡ºçš„ DuckDB æ•°æ®åº“æ–‡ä»¶è·¯å¾„ã€‚"
    )
    parser.add_argument(
        "--table-name",
        type=str,
        default="insurance_records",
        help="åœ¨ DuckDB ä¸­åˆ›å»ºçš„è¡¨åã€‚"
    )
    args = parser.parse_args()

    converter = ETLConverter(
        input_dir=args.input_dir,
        output_db=args.output_db,
        table_name=args.table_name
    )
    converter.run()

if __name__ == "__main__":
    main()
