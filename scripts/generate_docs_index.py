#!/usr/bin/env python3
"""
å¼€å‘æ–‡æ¡£ç´¢å¼•ç”Ÿæˆè„šæœ¬ - ä¸ºè½¦é™©æ•°æ®åˆ†æå¹³å°å®šåˆ¶
æ‰«æå¼€å‘æ–‡æ¡£ç›®å½•å¹¶ç”Ÿæˆå¢å¼ºçš„çŸ¥è¯†åº“ç´¢å¼•
"""

import os
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Any

class DocsIndexer:
    """æ–‡æ¡£ç´¢å¼•å™¨ - é€‚é…è½¦é™©é¡¹ç›®æ–‡æ¡£ç»“æ„"""

    def __init__(self, docs_dir: str):
        self.docs_dir = Path(docs_dir)
        self.stats = {
            'features': 0,
            'decisions': 0,
            'technical_docs': 0,
            'refactoring_docs': 0,
            'archived_docs': 0,
            'total_files': 0
        }
        self.index = {
            'features': [],
            'decisions': [],
            'technical': [],
            'refactoring': [],
            'recent_updates': []
        }
        # æ–°å¢ï¼šæ ‡ç­¾ç´¢å¼• {tag: [æ–‡æ¡£åˆ—è¡¨]}
        self.tags_index: Dict[str, List[Dict]] = {}
        # æ–°å¢ï¼šæ–‡æ¡£ä¾èµ–å…³ç³» {æ–‡æ¡£: [å®ƒå¼•ç”¨çš„æ–‡æ¡£åˆ—è¡¨]}
        self.dependencies: Dict[str, List[str]] = {}

    def extract_frontmatter(self, file_path: Path) -> Dict[str, Any]:
        """æå– YAML Frontmatter å…ƒæ•°æ®"""
        metadata = {
            'id': '',
            'title': '',
            'author': '',
            'status': '',
            'created_at': '',
            'updated_at': '',
            'tags': [],
            'domain': '',
            'complexity': ''
        }
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                lines = content.split('\n')
                
                if not lines or lines[0].strip() != '---':
                    return metadata
                
                for line in lines[1:]:
                    if line.strip() == '---':
                        break
                    
                    if ':' in line:
                        key, value = line.split(':', 1)
                        key = key.strip()
                        value = value.strip()
                        
                        if key == 'tags':
                            # å¤„ç† [tag1, tag2]
                            value = value.strip('[]')
                            tags = [t.strip().strip('"\'') for t in value.split(',') if t.strip()]
                            metadata['tags'] = tags
                        else:
                            metadata[key] = value
                            
        except Exception as e:
            pass
            
        return metadata

    def extract_title(self, file_path: Path, metadata: Dict = {}) -> str:
        """ä»Markdownæ–‡ä»¶æå–æ ‡é¢˜ï¼Œä¼˜å…ˆä½¿ç”¨ metadata"""
        if metadata and metadata.get('title'):
            return metadata['title']
            
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line.startswith('# '):
                        return line[2:].strip()
                    elif line.startswith('## '):
                        return line[3:].strip()
            return file_path.stem
        except:
            return file_path.stem

    def extract_summary(self, file_path: Path, max_lines: int = 5) -> str:
        """æå–æ–‡ä»¶çš„ç®€çŸ­æ‘˜è¦"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = []
                in_frontmatter = False
                skip_count = 0

                for line in f:
                    line = line.strip()

                    # è·³è¿‡YAML frontmatter
                    if line == '---':
                        if not in_frontmatter:
                            in_frontmatter = True
                            continue
                        else:
                            in_frontmatter = False
                            continue

                    if in_frontmatter:
                        continue

                    # è·³è¿‡æ ‡é¢˜è¡Œ
                    if line.startswith('#'):
                        skip_count += 1
                        if skip_count > 1:
                            continue
                        continue

                    # è·³è¿‡ç©ºè¡Œ
                    if not line:
                        continue

                    # è·³è¿‡åˆ†éš”çº¿
                    if line.startswith('---') or line.startswith('==='):
                        continue

                    # æ”¶é›†æœ‰æ•ˆå†…å®¹
                    if len(lines) < max_lines:
                        lines.append(line)
                    else:
                        break

                return ' '.join(lines)[:200] + '...' if lines else ''
        except:
            return ''

    def get_file_stats(self, file_path: Path, metadata: Dict = {}) -> Dict:
        """è·å–æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯ï¼Œä¼˜å…ˆä½¿ç”¨ metadata ä¸­çš„ updated_at"""
        stat = file_path.stat()
        stats = {
            'size': stat.st_size,
            'created': datetime.fromtimestamp(stat.st_ctime),
            'modified': datetime.fromtimestamp(stat.st_mtime)
        }
        
        if metadata and metadata.get('updated_at'):
            try:
                # å°è¯•è§£æ YYYY-MM-DD
                dt = datetime.strptime(metadata['updated_at'], '%Y-%m-%d')
                stats['modified'] = dt
            except:
                pass
                
        return stats

    def extract_tags(self, file_path: Path, metadata: Dict = {}) -> List[str]:
        """ä»æ–‡ä»¶ä¸­æå–æ ‡ç­¾ï¼ˆfrontmatter å’Œ hashtagsï¼‰"""
        tags = set()
        
        # 1. ä» metadata ä¸­è·å–
        if metadata and metadata.get('tags'):
            for tag in metadata['tags']:
                tags.add(tag)

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                
                # 2. æå–æ–‡æ¡£ä¸­çš„ hashtags (#æ ‡ç­¾)
                hashtag_pattern = re.compile(r'#(\w+[\u4e00-\u9fa5\w]*)')
                for match in hashtag_pattern.finditer(content):
                    tag = match.group(1)
                    # æ’é™¤ä¸€äº›å¸¸è§çš„éæ ‡ç­¾ç”¨æ³•ï¼ˆå¦‚æ ‡é¢˜ï¼‰
                    if not tag.isdigit():  # ä¸æ˜¯çº¯æ•°å­—
                        tags.add(tag)

        except Exception as e:
            pass

        return sorted(list(tags))

    def extract_links(self, file_path: Path) -> List[str]:
        """æå–æ–‡æ¡£ä¸­çš„æ‰€æœ‰é“¾æ¥"""
        links = []

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

                # æå– Markdown é“¾æ¥ [text](path)
                link_pattern = re.compile(r'\[([^\]]+)\]\(([^)]+)\)')
                for match in link_pattern.finditer(content):
                    link_path = match.group(2)

                    # åªä¿ç•™ç›¸å¯¹è·¯å¾„é“¾æ¥ï¼ˆæ–‡æ¡£å†…é“¾æ¥ï¼‰
                    if not link_path.startswith('http') and link_path.endswith('.md'):
                        # è§„èŒƒåŒ–è·¯å¾„
                        if link_path.startswith('./') or link_path.startswith('../'):
                            links.append(link_path)
                        else:
                            links.append(link_path)

        except Exception as e:
            pass

        return links
        
    def get_status_emoji(self, status: str) -> str:
        """è·å–çŠ¶æ€å¯¹åº”çš„ emoji"""
        status_map = {
            'stable': 'âœ…',
            'draft': 'ğŸš§',
            'review': 'ğŸ‘€',
            'deprecated': 'âŒ',
            'archived': 'ğŸ“¦'
        }
        return status_map.get(status.lower(), 'ğŸ“„') if status else 'ğŸ“„'

    def extract_related_code(self, file_path: Path, metadata: Dict = {}) -> List[str]:
        """æå–å…³è”ä»£ç  (related_code)"""
        code_files = []
        if metadata and metadata.get('related_code'):
            if isinstance(metadata['related_code'], list):
                code_files = metadata['related_code']
            elif isinstance(metadata['related_code'], str):
                code_files = [metadata['related_code']]
        return code_files

    def process_file(self, file_path: Path, category: str, category_list: List):
        """é€šç”¨æ–‡ä»¶å¤„ç†é€»è¾‘"""
        # æ˜ å°„ category åˆ° stats key
        stats_key_map = {
            'feature': 'features',
            'decision': 'decisions',
            'technical': 'technical_docs',
            'refactoring': 'refactoring_docs'
        }
        stats_key = stats_key_map.get(category)
        if stats_key:
            self.stats[stats_key] += 1

        self.stats['total_files'] += 1

        metadata = self.extract_frontmatter(file_path)
        title = self.extract_title(file_path, metadata)
        summary = self.extract_summary(file_path)
        stats = self.get_file_stats(file_path, metadata)
        tags = self.extract_tags(file_path, metadata)
        links = self.extract_links(file_path)
        related_code = self.extract_related_code(file_path, metadata)

        relative_path = str(file_path.relative_to(self.docs_dir))
        
        # ç¡®å®š ID
        doc_id = metadata.get('id') or file_path.stem

        status_value = metadata.get('status', '')
        entry = {
            'id': doc_id,
            'file': file_path.name,
            'title': title,
            'summary': summary,
            'path': relative_path,
            'modified': stats['modified'],
            'tags': tags,
            'status': status_value,
            'author': metadata.get('author', ''),
            'domain': metadata.get('domain', ''),
            'type': category,
            'related_code': related_code
        }

        category_list.append(entry)

        # æ›´æ–°æ ‡ç­¾ç´¢å¼•
        for tag in tags:
            if tag not in self.tags_index:
                self.tags_index[tag] = []
            self.tags_index[tag].append({
                'title': title,
                'path': relative_path,
                'type': category,
                'status': status_value
            })

        # æ›´æ–°ä¾èµ–å…³ç³»
        if links:
            self.dependencies[relative_path] = links

        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€è¿‘æ›´æ–°çš„
        if (datetime.now() - stats['modified']).days < 30:
            self.index['recent_updates'].append({
                'type': category,
                'title': title,
                'path': relative_path,
                'modified': stats['modified'],
                'status': status_value
            })

    def scan_features(self):
        """æ‰«æ01_featuresç›®å½•"""
        features_dir = self.docs_dir / '01_features'
        if not features_dir.exists():
            return

        for feature_dir in sorted(features_dir.iterdir()):
            if not feature_dir.is_dir():
                continue

            readme = feature_dir / 'README.md'
            if readme.exists():
                self.process_file(readme, 'feature', self.index['features'])

    def scan_decisions(self):
        """æ‰«æ02_decisionsç›®å½•"""
        decisions_dir = self.docs_dir / '02_decisions'
        if not decisions_dir.exists():
            return

        for md_file in sorted(decisions_dir.glob('*.md')):
            self.process_file(md_file, 'decision', self.index['decisions'])

    def scan_technical(self):
        """æ‰«æ03_technical_designç›®å½•"""
        tech_dir = self.docs_dir / '03_technical_design'
        if not tech_dir.exists():
            return

        for md_file in sorted(tech_dir.glob('*.md')):
            self.process_file(md_file, 'technical', self.index['technical'])

    def scan_refactoring(self):
        """æ‰«æ04_refactoringç›®å½•"""
        refactor_dir = self.docs_dir / '04_refactoring'
        if not refactor_dir.exists():
            return

        for md_file in sorted(refactor_dir.glob('*.md')):
            self.process_file(md_file, 'refactoring', self.index['refactoring'])

    def count_archived(self):
        """ç»Ÿè®¡å½’æ¡£æ–‡æ¡£æ•°é‡"""
        archive_dir = self.docs_dir / 'archive'
        if archive_dir.exists():
            self.stats['archived_docs'] = len(list(archive_dir.glob('*.md')))

    def generate_index_content(self) -> str:
        """ç”Ÿæˆç´¢å¼•å†…å®¹"""
        content = f'''# è½¦é™©æ•°æ®åˆ†æå¹³å° - çŸ¥è¯†åº“ç´¢å¼•

> ğŸ“… æœ€åæ›´æ–°: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
> ğŸ”„ è‡ªåŠ¨ç”Ÿæˆ by `scripts/generate_docs_index.py`

---

## ğŸ“Š çŸ¥è¯†åº“æ¦‚è§ˆ

| ç±»åˆ« | æ•°é‡ | è¯´æ˜ |
|------|------|------|
| ğŸ¯ åŠŸèƒ½æ¨¡å— | {self.stats['features']} | äº§å“åŠŸèƒ½æ–‡æ¡£ï¼ˆP0/P1/P2ä¼˜å…ˆçº§ï¼‰ |
| ğŸ—ï¸ æŠ€æœ¯å†³ç­– | {self.stats['decisions']} | ADRæ¶æ„å†³ç­–è®°å½• |
| âš™ï¸ æŠ€æœ¯è®¾è®¡ | {self.stats['technical_docs']} | æ•°æ®æ¶æ„ã€è®¡ç®—å…¬å¼ã€æŠ€æœ¯æ ˆ |
| ğŸ”§ é‡æ„æ–‡æ¡£ | {self.stats['refactoring_docs']} | æ¶æ„ä¼˜åŒ–å’Œé‡æ„è®¡åˆ’ |
| ğŸ“¦ å†å²å½’æ¡£ | {self.stats['archived_docs']} | æ—§ç‰ˆæœ¬æ–‡æ¡£å½’æ¡£ |
| **ğŸ“ æ€»è®¡** | **{self.stats['total_files']}** | **æ´»è·ƒæ–‡æ¡£æ€»æ•°** |

---

## ğŸ”¥ æœ€è¿‘æ›´æ–°ï¼ˆ30å¤©å†…ï¼‰

'''
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºæœ€è¿‘æ›´æ–°
        recent = sorted(self.index['recent_updates'], key=lambda x: x['modified'], reverse=True)

        if recent:
            for item in recent[:10]:  # æ˜¾ç¤ºæœ€è¿‘10ä¸ª
                days_ago = (datetime.now() - item['modified']).days
                time_str = f"{days_ago}å¤©å‰" if days_ago > 0 else "ä»Šå¤©"
                emoji_map = {
                    'feature': 'ğŸ¯',
                    'decision': 'ğŸ—ï¸',
                    'technical': 'âš™ï¸',
                    'refactoring': 'ğŸ”§'
                }
                emoji = emoji_map.get(item['type'], 'ğŸ“„')
                status_emoji = self.get_status_emoji(item.get('status'))
                
                content += f"- {emoji} {status_emoji} [{item['title']}]({item['path']}) - *{time_str}*\n"
        else:
            content += "*æš‚æ— æœ€è¿‘æ›´æ–°*\n"

        content += "\n---\n\n"

        # åŠŸèƒ½æ¨¡å—ç´¢å¼•
        content += "## ğŸ¯ åŠŸèƒ½æ¨¡å—æ–‡æ¡£\n\n"
        content += "> æŒ‰åŠŸèƒ½IDæ’åºï¼ŒåŒ…å«å¼€å‘çŠ¶æ€å’Œä¼˜å…ˆçº§\n\n"

        if self.index['features']:
            for feature in self.index['features']:
                # æå–ä¼˜å…ˆçº§ï¼ˆä»IDä¸­ï¼‰
                priority = "P0" if "F001" in feature['id'] or "F002" in feature['id'] or "F003" in feature['id'] or "F004" in feature['id'] else "P1/P2"
                status_emoji = self.get_status_emoji(feature.get('status'))
                
                content += f"### {status_emoji} [{feature['id']}] {feature['title']}\n\n"
                content += f"- **ä¼˜å…ˆçº§**: {priority}\n"
                if feature.get('status'):
                    content += f"- **çŠ¶æ€**: {feature['status']}\n"
                content += f"- **è·¯å¾„**: [`{feature['path']}`]({feature['path']})\n"
                if feature['summary']:
                    content += f"- **è¯´æ˜**: {feature['summary']}\n"
                content += f"- **æœ€åæ›´æ–°**: {feature['modified'].strftime('%Y-%m-%d')}\n\n"
        else:
            content += "*æš‚æ— åŠŸèƒ½æ–‡æ¡£*\n\n"

        content += "---\n\n"

        # æŠ€æœ¯å†³ç­–ç´¢å¼•
        content += "## ğŸ—ï¸ æŠ€æœ¯å†³ç­–è®°å½•ï¼ˆADRï¼‰\n\n"
        content += "> Architecture Decision Records - è®°å½•å…³é”®æŠ€æœ¯é€‰å‹å’Œè®¾è®¡å†³ç­–\n\n"

        if self.index['decisions']:
            content += "| çŠ¶æ€ | ADRç¼–å· | å†³ç­–æ ‡é¢˜ | æ‘˜è¦ | æ–‡æ¡£ |\n"
            content += "|------|---------|---------|------|------|\n"
            for decision in self.index['decisions']:
                # æå–ADRç¼–å·
                match = re.search(r'ADR-(\d+)', decision['file'])
                adr_num = match.group(1) if match else "N/A"
                summary_short = decision['summary'][:60] + '...' if len(decision['summary']) > 60 else decision['summary']
                status_emoji = self.get_status_emoji(decision.get('status'))
                
                content += f"| {status_emoji} | ADR-{adr_num} | {decision['title']} | {summary_short} | [`{decision['file']}`]({decision['path']}) |\n"
        else:
            content += "*æš‚æ— æŠ€æœ¯å†³ç­–æ–‡æ¡£*\n\n"

        content += "\n---\n\n"

        # æŠ€æœ¯è®¾è®¡æ–‡æ¡£
        content += "## âš™ï¸ æŠ€æœ¯è®¾è®¡æ–‡æ¡£\n\n"
        content += "> æ ¸å¿ƒæŠ€æœ¯æ¶æ„ã€æ•°æ®æ¨¡å‹ã€è®¡ç®—å…¬å¼ç­‰\n\n"

        if self.index['technical']:
            content += "| çŠ¶æ€ | åŸŸ | æ ‡é¢˜ | å†…å®¹ | è·¯å¾„ |\n"
            content += "|------|----|------|------|------|\n"
            for tech in self.index['technical']:
                status_emoji = self.get_status_emoji(tech.get('status'))
                domain = tech.get('domain', '-') or '-'
                summary_short = tech['summary'][:50] + '...' if len(tech['summary']) > 50 else tech['summary']
                
                content += f"| {status_emoji} | {domain} | {tech['title']} | {summary_short} | [`{tech['path']}`]({tech['path']}) |\n"
        else:
            content += "*æš‚æ— æŠ€æœ¯è®¾è®¡æ–‡æ¡£*\n\n"

        content += "\n---\n\n"

        # é‡æ„æ–‡æ¡£
        content += "## ğŸ”§ é‡æ„ä¸ä¼˜åŒ–æ–‡æ¡£\n\n"
        content += "> æ¶æ„æ¼”è¿›ã€ä»£ç é‡æ„è®¡åˆ’å’Œæœ€ä½³å®è·µ\n\n"

        if self.index['refactoring']:
            for refactor in self.index['refactoring']:
                status_emoji = self.get_status_emoji(refactor.get('status'))
                content += f"- {status_emoji} [{refactor['title']}]({refactor['path']})\n"
        else:
            content += "*æš‚æ— é‡æ„æ–‡æ¡£*\n\n"

        content += "\n---\n\n"

        # æ ‡ç­¾ç´¢å¼•
        content += "## ğŸ·ï¸ æ ‡ç­¾ç´¢å¼•\n\n"
        content += "> æŒ‰æ ‡ç­¾å¿«é€ŸæŸ¥æ‰¾ç›¸å…³æ–‡æ¡£\n\n"

        if self.tags_index:
            # æŒ‰æ ‡ç­¾æ–‡æ¡£æ•°é‡æ’åº
            sorted_tags = sorted(self.tags_index.items(), key=lambda x: len(x[1]), reverse=True)

            # æ˜¾ç¤ºçƒ­é—¨æ ‡ç­¾ï¼ˆæ–‡æ¡£æ•° >= 2ï¼‰
            popular_tags = [(tag, docs) for tag, docs in sorted_tags if len(docs) >= 2]

            if popular_tags:
                content += "### çƒ­é—¨æ ‡ç­¾\n\n"
                for tag, docs in popular_tags[:15]:  # æ˜¾ç¤ºå‰15ä¸ªçƒ­é—¨æ ‡ç­¾
                    content += f"**#{tag}** ({len(docs)}ä¸ªæ–‡æ¡£)\n"
                    for doc in docs:
                        emoji_map = {
                            'feature': 'ğŸ¯',
                            'decision': 'ğŸ—ï¸',
                            'technical': 'âš™ï¸',
                            'refactoring': 'ğŸ”§'
                        }
                        emoji = emoji_map.get(doc['type'], 'ğŸ“„')
                        status_emoji = self.get_status_emoji(doc.get('status') or '')
                        content += f"- {emoji} {status_emoji} [{doc['title']}]({doc['path']})\n"
                    content += "\n"

            # æ‰€æœ‰æ ‡ç­¾ï¼ˆå­—æ¯åºï¼‰
            content += "### æ‰€æœ‰æ ‡ç­¾\n\n"
            content += "| æ ‡ç­¾ | æ–‡æ¡£æ•° | æ–‡æ¡£åˆ—è¡¨ |\n"
            content += "|------|--------|----------|\n"

            for tag, docs in sorted(sorted_tags, key=lambda x: x[0]):
                doc_links = ', '.join([f"[{doc['title']}]({doc['path']})" for doc in docs[:3]])
                if len(docs) > 3:
                    doc_links += f" ç­‰{len(docs)}ä¸ª"
                content += f"| #{tag} | {len(docs)} | {doc_links} |\n"

            content += "\n"
        else:
            content += "*æš‚æ— æ ‡ç­¾*\n\n"

        content += "---\n\n"

        # æ–‡æ¡£ä¾èµ–å…³ç³»å›¾
        content += "## ğŸ”— æ–‡æ¡£ä¾èµ–å…³ç³»å›¾\n\n"
        content += "> æ˜¾ç¤ºæ–‡æ¡£ä¹‹é—´çš„å¼•ç”¨å…³ç³»\n\n"

        if self.dependencies:
            # ç»Ÿè®¡è¢«å¼•ç”¨æœ€å¤šçš„æ–‡æ¡£
            referenced_count: Dict[str, int] = {}
            for source, targets in self.dependencies.items():
                for target in targets:
                    # è§„èŒƒåŒ–è·¯å¾„
                    normalized = target.replace('../', '').replace('./', '')
                    referenced_count[normalized] = referenced_count.get(normalized, 0) + 1

            # æ˜¾ç¤ºæ ¸å¿ƒæ–‡æ¡£ï¼ˆè¢«å¼•ç”¨3æ¬¡ä»¥ä¸Šï¼‰
            core_docs = [(path, count) for path, count in referenced_count.items() if count >= 3]

            if core_docs:
                content += "### ğŸŒŸ æ ¸å¿ƒæ–‡æ¡£ï¼ˆè¢«å¼•ç”¨â‰¥3æ¬¡ï¼‰\n\n"
                for path, count in sorted(core_docs, key=lambda x: x[1], reverse=True):
                    content += f"- `{path}` - è¢«å¼•ç”¨ **{count}** æ¬¡\n"
                content += "\n"

            # æ˜¾ç¤ºå¼•ç”¨å…³ç³»
            content += "### æ–‡æ¡£å¼•ç”¨å…³ç³»\n\n"
            content += "<details>\n<summary>ç‚¹å‡»å±•å¼€å®Œæ•´å¼•ç”¨å…³ç³»</summary>\n\n"

            for source, targets in sorted(self.dependencies.items()):
                content += f"**{source}** å¼•ç”¨:\n"
                for target in targets:
                    content += f"  - `{target}`\n"
                content += "\n"

            content += "</details>\n\n"
        else:
            content += "*æš‚æ— æ–‡æ¡£å¼•ç”¨å…³ç³»*\n\n"

        content += "---\n\n"

        # ä½¿ç”¨æŒ‡å—
        content += '''## ğŸ“– ä½¿ç”¨æŒ‡å—

### å¿«é€Ÿå¯¼èˆª

1. **æ–°æ‰‹å…¥é—¨** â†’ é˜…è¯» [README.md](README.md) äº†è§£é¡¹ç›®æ¦‚è§ˆ
2. **å¼€å‘åä½œ** â†’ æŸ¥çœ‹ [00_conventions.md](00_conventions.md) ç†è§£"ä»£ç ä¼˜å…ˆ"åŸåˆ™
3. **åŠŸèƒ½å¼€å‘** â†’ æµè§ˆ `01_features/` ç›®å½•æ‰¾åˆ°å¯¹åº”åŠŸèƒ½æ–‡æ¡£
4. **æŠ€æœ¯é€‰å‹** â†’ å‚è€ƒ `02_decisions/` ä¸­çš„ADRæ–‡æ¡£
5. **æ¶æ„è®¾è®¡** â†’ æŸ¥é˜… `03_technical_design/` äº†è§£æŠ€æœ¯æ¶æ„
6. **å†å²æŸ¥è¯¢** â†’ æœç´¢ `archive/` ç›®å½•æŸ¥æ‰¾æ—§ç‰ˆæœ¬æ–‡æ¡£

### æ–‡æ¡£ç»´æŠ¤è§„èŒƒ

âœ… **å¿…é¡»åšçš„äº‹æƒ…**ï¼š
- ä»£ç å˜æ›´åç«‹å³æ›´æ–°å¯¹åº”åŠŸèƒ½æ–‡æ¡£
- é‡å¤§æŠ€æœ¯å†³ç­–åˆ›å»ºæ–°çš„ADRæ–‡æ¡£
- æ¯æ¬¡åŠŸèƒ½å‘å¸ƒå‰è¿è¡Œ `python scripts/generate_docs_index.py å¼€å‘æ–‡æ¡£`

âŒ **ç¦æ­¢åšçš„äº‹æƒ…**ï¼š
- åŸºäº"è®°å¿†"è€Œéä»£ç æ ‡è®°åŠŸèƒ½çŠ¶æ€
- ä¿ç•™ä¸ä»£ç å®ç°ä¸ç¬¦çš„è¿‡æœŸæ–‡æ¡£
- ç›´æ¥ä¿®æ”¹è‡ªåŠ¨ç”Ÿæˆçš„ç´¢å¼•æ–‡ä»¶

### æ›´æ–°ç´¢å¼•

```bash
# æ‰«æå¼€å‘æ–‡æ¡£å¹¶é‡æ–°ç”Ÿæˆç´¢å¼•
python scripts/generate_docs_index.py å¼€å‘æ–‡æ¡£
```
'''
        return content

    def run(self):
        """æ‰§è¡Œç´¢å¼•ç”Ÿæˆæµç¨‹"""
        print(f"æ­£åœ¨æ‰«ææ–‡æ¡£ç›®å½•: {self.docs_dir}")
        
        self.scan_features()
        self.scan_decisions()
        self.scan_technical()
        self.scan_refactoring()
        self.count_archived()
        
        content = self.generate_index_content()
        
        output_file = self.docs_dir / 'KNOWLEDGE_INDEX.md'
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(content)
            
        print(f"ç´¢å¼•ç”Ÿæˆå®Œæˆ! å·²ä¿å­˜è‡³: {output_file}")
        print(f"æ€»è®¡æ‰«ææ–‡ä»¶: {self.stats['total_files']}")

if __name__ == '__main__':
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python generate_docs_index.py <docs_dir>")
        sys.exit(1)
        
    docs_dir = sys.argv[1]
    indexer = DocsIndexer(docs_dir)
    indexer.run()
