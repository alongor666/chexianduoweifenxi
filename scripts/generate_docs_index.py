#!/usr/bin/env python3
"""
å¼€å‘æ–‡æ¡£ç´¢å¼•ç”Ÿæˆè„šæœ¬ - ä¸ºè½¦é™©æ•°æ®åˆ†æå¹³å°å®šåˆ¶
æ‰«æå¼€å‘æ–‡æ¡£ç›®å½•å¹¶ç”Ÿæˆå¢å¼ºçš„çŸ¥è¯†åº“ç´¢å¼•
"""

import os
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple

class DocsIndexer:
    """æ–‡æ¡£ç´¢å¼•å™¨ - é€‚é…è½¦é™©é¡¹ç›®æ–‡æ¡£ç»“æ„"""

    def __init__(self, docs_dir: str):
        self.docs_dir = Path(docs_dir)
        self.stats = {
            'features': 0,
            'decisions': 0,
            'technical_docs': 0,
            'refactoring_docs': 0,
            'archived_docs': 0,
            'total_files': 0
        }
        self.index = {
            'features': [],
            'decisions': [],
            'technical': [],
            'refactoring': [],
            'recent_updates': []
        }
        # æ–°å¢ï¼šæ ‡ç­¾ç´¢å¼• {tag: [æ–‡æ¡£åˆ—è¡¨]}
        self.tags_index: Dict[str, List[Dict]] = {}
        # æ–°å¢ï¼šæ–‡æ¡£ä¾èµ–å…³ç³» {æ–‡æ¡£: [å®ƒå¼•ç”¨çš„æ–‡æ¡£åˆ—è¡¨]}
        self.dependencies: Dict[str, List[str]] = {}

    def extract_title(self, file_path: Path) -> str:
        """ä»Markdownæ–‡ä»¶æå–æ ‡é¢˜"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line.startswith('# '):
                        return line[2:].strip()
                    elif line.startswith('## '):
                        return line[3:].strip()
            return file_path.stem
        except:
            return file_path.stem

    def extract_summary(self, file_path: Path, max_lines: int = 5) -> str:
        """æå–æ–‡ä»¶çš„ç®€çŸ­æ‘˜è¦"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = []
                in_frontmatter = False
                skip_count = 0

                for line in f:
                    line = line.strip()

                    # è·³è¿‡YAML frontmatter
                    if line == '---':
                        if not in_frontmatter:
                            in_frontmatter = True
                            continue
                        else:
                            in_frontmatter = False
                            continue

                    if in_frontmatter:
                        continue

                    # è·³è¿‡æ ‡é¢˜è¡Œ
                    if line.startswith('#'):
                        skip_count += 1
                        if skip_count > 1:
                            continue
                        continue

                    # è·³è¿‡ç©ºè¡Œ
                    if not line:
                        continue

                    # è·³è¿‡åˆ†éš”çº¿
                    if line.startswith('---') or line.startswith('==='):
                        continue

                    # æ”¶é›†æœ‰æ•ˆå†…å®¹
                    if len(lines) < max_lines:
                        lines.append(line)
                    else:
                        break

                return ' '.join(lines)[:200] + '...' if lines else ''
        except:
            return ''

    def get_file_stats(self, file_path: Path) -> Dict:
        """è·å–æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯"""
        stat = file_path.stat()
        return {
            'size': stat.st_size,
            'created': datetime.fromtimestamp(stat.st_ctime),
            'modified': datetime.fromtimestamp(stat.st_mtime)
        }

    def extract_tags(self, file_path: Path) -> List[str]:
        """ä»æ–‡ä»¶ä¸­æå–æ ‡ç­¾ï¼ˆfrontmatter å’Œ hashtagsï¼‰"""
        tags = set()

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                lines = content.split('\n')

                # 1. æå– YAML frontmatter ä¸­çš„æ ‡ç­¾
                if lines and lines[0].strip() == '---':
                    in_frontmatter = True
                    for i, line in enumerate(lines[1:], 1):
                        if line.strip() == '---':
                            break
                        # tags: [tag1, tag2] æˆ– tags: tag1, tag2
                        if line.strip().startswith('tags:'):
                            tags_str = line.split(':', 1)[1].strip()
                            # ç§»é™¤æ–¹æ‹¬å·
                            tags_str = tags_str.strip('[]')
                            # åˆ†å‰²å¹¶æ¸…ç†
                            for tag in tags_str.split(','):
                                tag = tag.strip().strip('"\'')
                                if tag:
                                    tags.add(tag)

                # 2. æå–æ–‡æ¡£ä¸­çš„ hashtags (#æ ‡ç­¾)
                hashtag_pattern = re.compile(r'#(\w+[\u4e00-\u9fa5\w]*)')
                for match in hashtag_pattern.finditer(content):
                    tag = match.group(1)
                    # æ’é™¤ä¸€äº›å¸¸è§çš„éæ ‡ç­¾ç”¨æ³•ï¼ˆå¦‚æ ‡é¢˜ï¼‰
                    if not tag.isdigit():  # ä¸æ˜¯çº¯æ•°å­—
                        tags.add(tag)

        except Exception as e:
            pass

        return sorted(list(tags))

    def extract_links(self, file_path: Path) -> List[str]:
        """æå–æ–‡æ¡£ä¸­çš„æ‰€æœ‰é“¾æ¥"""
        links = []

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

                # æå– Markdown é“¾æ¥ [text](path)
                link_pattern = re.compile(r'\[([^\]]+)\]\(([^)]+)\)')
                for match in link_pattern.finditer(content):
                    link_path = match.group(2)

                    # åªä¿ç•™ç›¸å¯¹è·¯å¾„é“¾æ¥ï¼ˆæ–‡æ¡£å†…é“¾æ¥ï¼‰
                    if not link_path.startswith('http') and link_path.endswith('.md'):
                        # è§„èŒƒåŒ–è·¯å¾„
                        if link_path.startswith('./') or link_path.startswith('../'):
                            links.append(link_path)
                        else:
                            links.append(link_path)

        except Exception as e:
            pass

        return links

    def scan_features(self):
        """æ‰«æ01_featuresç›®å½•"""
        features_dir = self.docs_dir / '01_features'
        if not features_dir.exists():
            return

        for feature_dir in sorted(features_dir.iterdir()):
            if not feature_dir.is_dir():
                continue

            readme = feature_dir / 'README.md'
            if readme.exists():
                self.stats['features'] += 1
                self.stats['total_files'] += 1

                title = self.extract_title(readme)
                summary = self.extract_summary(readme)
                stats = self.get_file_stats(readme)
                tags = self.extract_tags(readme)
                links = self.extract_links(readme)

                relative_path = str(readme.relative_to(self.docs_dir))

                feature_entry = {
                    'id': feature_dir.name,
                    'title': title,
                    'summary': summary,
                    'path': relative_path,
                    'modified': stats['modified'],
                    'tags': tags
                }

                self.index['features'].append(feature_entry)

                # æ›´æ–°æ ‡ç­¾ç´¢å¼•
                for tag in tags:
                    if tag not in self.tags_index:
                        self.tags_index[tag] = []
                    self.tags_index[tag].append({
                        'title': title,
                        'path': relative_path,
                        'type': 'feature'
                    })

                # æ›´æ–°ä¾èµ–å…³ç³»
                if links:
                    self.dependencies[relative_path] = links

                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€è¿‘æ›´æ–°çš„
                if (datetime.now() - stats['modified']).days < 30:
                    self.index['recent_updates'].append({
                        'type': 'feature',
                        'title': title,
                        'path': relative_path,
                        'modified': stats['modified']
                    })

    def scan_decisions(self):
        """æ‰«æ02_decisionsç›®å½•"""
        decisions_dir = self.docs_dir / '02_decisions'
        if not decisions_dir.exists():
            return

        for md_file in sorted(decisions_dir.glob('*.md')):
            self.stats['decisions'] += 1
            self.stats['total_files'] += 1

            title = self.extract_title(md_file)
            summary = self.extract_summary(md_file)
            stats = self.get_file_stats(md_file)
            tags = self.extract_tags(md_file)
            links = self.extract_links(md_file)

            relative_path = str(md_file.relative_to(self.docs_dir))

            decision_entry = {
                'file': md_file.name,
                'title': title,
                'summary': summary,
                'path': relative_path,
                'modified': stats['modified'],
                'tags': tags
            }

            self.index['decisions'].append(decision_entry)

            # æ›´æ–°æ ‡ç­¾ç´¢å¼•
            for tag in tags:
                if tag not in self.tags_index:
                    self.tags_index[tag] = []
                self.tags_index[tag].append({
                    'title': title,
                    'path': relative_path,
                    'type': 'decision'
                })

            # æ›´æ–°ä¾èµ–å…³ç³»
            if links:
                self.dependencies[relative_path] = links

            if (datetime.now() - stats['modified']).days < 30:
                self.index['recent_updates'].append({
                    'type': 'decision',
                    'title': title,
                    'path': relative_path,
                    'modified': stats['modified']
                })

    def scan_technical(self):
        """æ‰«æ03_technical_designç›®å½•"""
        tech_dir = self.docs_dir / '03_technical_design'
        if not tech_dir.exists():
            return

        for md_file in sorted(tech_dir.glob('*.md')):
            self.stats['technical_docs'] += 1
            self.stats['total_files'] += 1

            title = self.extract_title(md_file)
            summary = self.extract_summary(md_file)
            stats = self.get_file_stats(md_file)
            tags = self.extract_tags(md_file)
            links = self.extract_links(md_file)

            relative_path = str(md_file.relative_to(self.docs_dir))

            tech_entry = {
                'file': md_file.name,
                'title': title,
                'summary': summary,
                'path': relative_path,
                'modified': stats['modified'],
                'tags': tags
            }

            self.index['technical'].append(tech_entry)

            # æ›´æ–°æ ‡ç­¾ç´¢å¼•
            for tag in tags:
                if tag not in self.tags_index:
                    self.tags_index[tag] = []
                self.tags_index[tag].append({
                    'title': title,
                    'path': relative_path,
                    'type': 'technical'
                })

            # æ›´æ–°ä¾èµ–å…³ç³»
            if links:
                self.dependencies[relative_path] = links

            if (datetime.now() - stats['modified']).days < 30:
                self.index['recent_updates'].append({
                    'type': 'technical',
                    'title': title,
                    'path': relative_path,
                    'modified': stats['modified']
                })

    def scan_refactoring(self):
        """æ‰«æ04_refactoringç›®å½•"""
        refactor_dir = self.docs_dir / '04_refactoring'
        if not refactor_dir.exists():
            return

        for md_file in sorted(refactor_dir.glob('*.md')):
            self.stats['refactoring_docs'] += 1
            self.stats['total_files'] += 1

            title = self.extract_title(md_file)
            summary = self.extract_summary(md_file)
            stats = self.get_file_stats(md_file)
            tags = self.extract_tags(md_file)
            links = self.extract_links(md_file)

            relative_path = str(md_file.relative_to(self.docs_dir))

            refactor_entry = {
                'file': md_file.name,
                'title': title,
                'summary': summary,
                'path': relative_path,
                'modified': stats['modified'],
                'tags': tags
            }

            self.index['refactoring'].append(refactor_entry)

            # æ›´æ–°æ ‡ç­¾ç´¢å¼•
            for tag in tags:
                if tag not in self.tags_index:
                    self.tags_index[tag] = []
                self.tags_index[tag].append({
                    'title': title,
                    'path': relative_path,
                    'type': 'refactoring'
                })

            # æ›´æ–°ä¾èµ–å…³ç³»
            if links:
                self.dependencies[relative_path] = links

    def count_archived(self):
        """ç»Ÿè®¡å½’æ¡£æ–‡æ¡£æ•°é‡"""
        archive_dir = self.docs_dir / 'archive'
        if archive_dir.exists():
            self.stats['archived_docs'] = len(list(archive_dir.glob('*.md')))

    def generate_index_content(self) -> str:
        """ç”Ÿæˆç´¢å¼•å†…å®¹"""
        content = f'''# è½¦é™©æ•°æ®åˆ†æå¹³å° - çŸ¥è¯†åº“ç´¢å¼•

> ğŸ“… æœ€åæ›´æ–°: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
> ğŸ”„ è‡ªåŠ¨ç”Ÿæˆ by `scripts/generate_docs_index.py`

---

## ğŸ“Š çŸ¥è¯†åº“æ¦‚è§ˆ

| ç±»åˆ« | æ•°é‡ | è¯´æ˜ |
|------|------|------|
| ğŸ¯ åŠŸèƒ½æ¨¡å— | {self.stats['features']} | äº§å“åŠŸèƒ½æ–‡æ¡£ï¼ˆP0/P1/P2ä¼˜å…ˆçº§ï¼‰ |
| ğŸ—ï¸ æŠ€æœ¯å†³ç­– | {self.stats['decisions']} | ADRæ¶æ„å†³ç­–è®°å½• |
| âš™ï¸ æŠ€æœ¯è®¾è®¡ | {self.stats['technical_docs']} | æ•°æ®æ¶æ„ã€è®¡ç®—å…¬å¼ã€æŠ€æœ¯æ ˆ |
| ğŸ”§ é‡æ„æ–‡æ¡£ | {self.stats['refactoring_docs']} | æ¶æ„ä¼˜åŒ–å’Œé‡æ„è®¡åˆ’ |
| ğŸ“¦ å†å²å½’æ¡£ | {self.stats['archived_docs']} | æ—§ç‰ˆæœ¬æ–‡æ¡£å½’æ¡£ |
| **ğŸ“ æ€»è®¡** | **{self.stats['total_files']}** | **æ´»è·ƒæ–‡æ¡£æ€»æ•°** |

---

## ğŸ”¥ æœ€è¿‘æ›´æ–°ï¼ˆ30å¤©å†…ï¼‰

'''
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºæœ€è¿‘æ›´æ–°
        recent = sorted(self.index['recent_updates'], key=lambda x: x['modified'], reverse=True)

        if recent:
            for item in recent[:10]:  # æ˜¾ç¤ºæœ€è¿‘10ä¸ª
                days_ago = (datetime.now() - item['modified']).days
                time_str = f"{days_ago}å¤©å‰" if days_ago > 0 else "ä»Šå¤©"
                emoji_map = {
                    'feature': 'ğŸ¯',
                    'decision': 'ğŸ—ï¸',
                    'technical': 'âš™ï¸'
                }
                emoji = emoji_map.get(item['type'], 'ğŸ“„')
                content += f"- {emoji} [{item['title']}]({item['path']}) - *{time_str}*\n"
        else:
            content += "*æš‚æ— æœ€è¿‘æ›´æ–°*\n"

        content += "\n---\n\n"

        # åŠŸèƒ½æ¨¡å—ç´¢å¼•
        content += "## ğŸ¯ åŠŸèƒ½æ¨¡å—æ–‡æ¡£\n\n"
        content += "> æŒ‰åŠŸèƒ½IDæ’åºï¼ŒåŒ…å«å¼€å‘çŠ¶æ€å’Œä¼˜å…ˆçº§\n\n"

        if self.index['features']:
            for feature in self.index['features']:
                # æå–ä¼˜å…ˆçº§ï¼ˆä»IDä¸­ï¼‰
                priority = "P0" if "F001" in feature['id'] or "F002" in feature['id'] or "F003" in feature['id'] or "F004" in feature['id'] else "P1/P2"
                content += f"### [{feature['id']}] {feature['title']}\n\n"
                content += f"- **ä¼˜å…ˆçº§**: {priority}\n"
                content += f"- **è·¯å¾„**: [`{feature['path']}`]({feature['path']})\n"
                if feature['summary']:
                    content += f"- **è¯´æ˜**: {feature['summary']}\n"
                content += f"- **æœ€åæ›´æ–°**: {feature['modified'].strftime('%Y-%m-%d')}\n\n"
        else:
            content += "*æš‚æ— åŠŸèƒ½æ–‡æ¡£*\n\n"

        content += "---\n\n"

        # æŠ€æœ¯å†³ç­–ç´¢å¼•
        content += "## ğŸ—ï¸ æŠ€æœ¯å†³ç­–è®°å½•ï¼ˆADRï¼‰\n\n"
        content += "> Architecture Decision Records - è®°å½•å…³é”®æŠ€æœ¯é€‰å‹å’Œè®¾è®¡å†³ç­–\n\n"

        if self.index['decisions']:
            content += "| ADRç¼–å· | å†³ç­–æ ‡é¢˜ | æ‘˜è¦ | æ–‡æ¡£ |\n"
            content += "|---------|---------|------|------|\n"
            for decision in self.index['decisions']:
                # æå–ADRç¼–å·
                match = re.search(r'ADR-(\d+)', decision['file'])
                adr_num = match.group(1) if match else "N/A"
                summary_short = decision['summary'][:80] + '...' if len(decision['summary']) > 80 else decision['summary']
                content += f"| ADR-{adr_num} | {decision['title']} | {summary_short} | [`{decision['file']}`]({decision['path']}) |\n"
        else:
            content += "*æš‚æ— æŠ€æœ¯å†³ç­–æ–‡æ¡£*\n\n"

        content += "\n---\n\n"

        # æŠ€æœ¯è®¾è®¡æ–‡æ¡£
        content += "## âš™ï¸ æŠ€æœ¯è®¾è®¡æ–‡æ¡£\n\n"
        content += "> æ ¸å¿ƒæŠ€æœ¯æ¶æ„ã€æ•°æ®æ¨¡å‹ã€è®¡ç®—å…¬å¼ç­‰\n\n"

        if self.index['technical']:
            for tech in self.index['technical']:
                content += f"### {tech['title']}\n\n"
                content += f"- **è·¯å¾„**: [`{tech['path']}`]({tech['path']})\n"
                if tech['summary']:
                    content += f"- **å†…å®¹**: {tech['summary']}\n"
                content += f"- **æœ€åæ›´æ–°**: {tech['modified'].strftime('%Y-%m-%d')}\n\n"
        else:
            content += "*æš‚æ— æŠ€æœ¯è®¾è®¡æ–‡æ¡£*\n\n"

        content += "---\n\n"

        # é‡æ„æ–‡æ¡£
        content += "## ğŸ”§ é‡æ„ä¸ä¼˜åŒ–æ–‡æ¡£\n\n"
        content += "> æ¶æ„æ¼”è¿›ã€ä»£ç é‡æ„è®¡åˆ’å’Œæœ€ä½³å®è·µ\n\n"

        if self.index['refactoring']:
            for refactor in self.index['refactoring']:
                content += f"- [{refactor['title']}]({refactor['path']})\n"
        else:
            content += "*æš‚æ— é‡æ„æ–‡æ¡£*\n\n"

        content += "\n---\n\n"

        # æ ‡ç­¾ç´¢å¼•
        content += "## ğŸ·ï¸ æ ‡ç­¾ç´¢å¼•\n\n"
        content += "> æŒ‰æ ‡ç­¾å¿«é€ŸæŸ¥æ‰¾ç›¸å…³æ–‡æ¡£\n\n"

        if self.tags_index:
            # æŒ‰æ ‡ç­¾æ–‡æ¡£æ•°é‡æ’åº
            sorted_tags = sorted(self.tags_index.items(), key=lambda x: len(x[1]), reverse=True)

            # æ˜¾ç¤ºçƒ­é—¨æ ‡ç­¾ï¼ˆæ–‡æ¡£æ•° >= 2ï¼‰
            popular_tags = [(tag, docs) for tag, docs in sorted_tags if len(docs) >= 2]

            if popular_tags:
                content += "### çƒ­é—¨æ ‡ç­¾\n\n"
                for tag, docs in popular_tags[:15]:  # æ˜¾ç¤ºå‰15ä¸ªçƒ­é—¨æ ‡ç­¾
                    content += f"**#{tag}** ({len(docs)}ä¸ªæ–‡æ¡£)\n"
                    for doc in docs:
                        emoji_map = {
                            'feature': 'ğŸ¯',
                            'decision': 'ğŸ—ï¸',
                            'technical': 'âš™ï¸',
                            'refactoring': 'ğŸ”§'
                        }
                        emoji = emoji_map.get(doc['type'], 'ğŸ“„')
                        content += f"- {emoji} [{doc['title']}]({doc['path']})\n"
                    content += "\n"

            # æ‰€æœ‰æ ‡ç­¾ï¼ˆå­—æ¯åºï¼‰
            content += "### æ‰€æœ‰æ ‡ç­¾\n\n"
            content += "| æ ‡ç­¾ | æ–‡æ¡£æ•° | æ–‡æ¡£åˆ—è¡¨ |\n"
            content += "|------|--------|----------|\n"

            for tag, docs in sorted(sorted_tags, key=lambda x: x[0]):
                doc_links = ', '.join([f"[{doc['title']}]({doc['path']})" for doc in docs[:3]])
                if len(docs) > 3:
                    doc_links += f" ç­‰{len(docs)}ä¸ª"
                content += f"| #{tag} | {len(docs)} | {doc_links} |\n"

            content += "\n"
        else:
            content += "*æš‚æ— æ ‡ç­¾*\n\n"

        content += "---\n\n"

        # æ–‡æ¡£ä¾èµ–å…³ç³»å›¾
        content += "## ğŸ”— æ–‡æ¡£ä¾èµ–å…³ç³»å›¾\n\n"
        content += "> æ˜¾ç¤ºæ–‡æ¡£ä¹‹é—´çš„å¼•ç”¨å…³ç³»\n\n"

        if self.dependencies:
            # ç»Ÿè®¡è¢«å¼•ç”¨æœ€å¤šçš„æ–‡æ¡£
            referenced_count: Dict[str, int] = {}
            for source, targets in self.dependencies.items():
                for target in targets:
                    # è§„èŒƒåŒ–è·¯å¾„
                    normalized = target.replace('../', '').replace('./', '')
                    referenced_count[normalized] = referenced_count.get(normalized, 0) + 1

            # æ˜¾ç¤ºæ ¸å¿ƒæ–‡æ¡£ï¼ˆè¢«å¼•ç”¨3æ¬¡ä»¥ä¸Šï¼‰
            core_docs = [(path, count) for path, count in referenced_count.items() if count >= 3]

            if core_docs:
                content += "### ğŸŒŸ æ ¸å¿ƒæ–‡æ¡£ï¼ˆè¢«å¼•ç”¨â‰¥3æ¬¡ï¼‰\n\n"
                for path, count in sorted(core_docs, key=lambda x: x[1], reverse=True):
                    content += f"- `{path}` - è¢«å¼•ç”¨ **{count}** æ¬¡\n"
                content += "\n"

            # æ˜¾ç¤ºå¼•ç”¨å…³ç³»
            content += "### æ–‡æ¡£å¼•ç”¨å…³ç³»\n\n"
            content += "<details>\n<summary>ç‚¹å‡»å±•å¼€å®Œæ•´å¼•ç”¨å…³ç³»</summary>\n\n"

            for source, targets in sorted(self.dependencies.items()):
                content += f"**{source}** å¼•ç”¨:\n"
                for target in targets:
                    content += f"  - `{target}`\n"
                content += "\n"

            content += "</details>\n\n"
        else:
            content += "*æš‚æ— æ–‡æ¡£å¼•ç”¨å…³ç³»*\n\n"

        content += "---\n\n"

        # ä½¿ç”¨æŒ‡å—
        content += '''## ğŸ“– ä½¿ç”¨æŒ‡å—

### å¿«é€Ÿå¯¼èˆª

1. **æ–°æ‰‹å…¥é—¨** â†’ é˜…è¯» [README.md](README.md) äº†è§£é¡¹ç›®æ¦‚è§ˆ
2. **å¼€å‘åä½œ** â†’ æŸ¥çœ‹ [00_conventions.md](00_conventions.md) ç†è§£"ä»£ç ä¼˜å…ˆ"åŸåˆ™
3. **åŠŸèƒ½å¼€å‘** â†’ æµè§ˆ `01_features/` ç›®å½•æ‰¾åˆ°å¯¹åº”åŠŸèƒ½æ–‡æ¡£
4. **æŠ€æœ¯é€‰å‹** â†’ å‚è€ƒ `02_decisions/` ä¸­çš„ADRæ–‡æ¡£
5. **æ¶æ„è®¾è®¡** â†’ æŸ¥é˜… `03_technical_design/` äº†è§£æŠ€æœ¯æ¶æ„
6. **å†å²æŸ¥è¯¢** â†’ æœç´¢ `archive/` ç›®å½•æŸ¥æ‰¾æ—§ç‰ˆæœ¬æ–‡æ¡£

### æ–‡æ¡£ç»´æŠ¤è§„èŒƒ

âœ… **å¿…é¡»åšçš„äº‹æƒ…**ï¼š
- ä»£ç å˜æ›´åç«‹å³æ›´æ–°å¯¹åº”åŠŸèƒ½æ–‡æ¡£
- é‡å¤§æŠ€æœ¯å†³ç­–åˆ›å»ºæ–°çš„ADRæ–‡æ¡£
- æ¯æ¬¡åŠŸèƒ½å‘å¸ƒå‰è¿è¡Œ `python scripts/generate_docs_index.py å¼€å‘æ–‡æ¡£`

âŒ **ç¦æ­¢åšçš„äº‹æƒ…**ï¼š
- åŸºäº"è®°å¿†"è€Œéä»£ç æ ‡è®°åŠŸèƒ½çŠ¶æ€
- ä¿ç•™ä¸ä»£ç å®ç°ä¸ç¬¦çš„è¿‡æœŸæ–‡æ¡£
- ç›´æ¥ä¿®æ”¹è‡ªåŠ¨ç”Ÿæˆçš„ç´¢å¼•æ–‡ä»¶

### æ›´æ–°ç´¢å¼•

```bash
# æ‰«æå¼€å‘æ–‡æ¡£å¹¶é‡æ–°ç”Ÿæˆç´¢å¼•
python scripts/generate_docs_index.py å¼€å‘æ–‡æ¡£

# æˆ–ä½¿ç”¨ç›¸å¯¹è·¯å¾„
cd scripts
python generate_docs_index.py ../å¼€å‘æ–‡æ¡£
```

---

## ğŸ”— ç›¸å…³èµ„æº

- **é¡¹ç›®ä¸»é¡µ**: [../README.md](../README.md)
- **AIåä½œæŒ‡å—**: [../CLAUDE.md](../CLAUDE.md)
- **å¼€å‘çº¦å®š**: [00_conventions.md](00_conventions.md)
- **å†å²å½’æ¡£**: [archive/](archive/)

---

*æœ¬ç´¢å¼•ç”± `scripts/generate_docs_index.py` è‡ªåŠ¨ç”Ÿæˆ*
*å¦‚éœ€æ›´æ–°ï¼Œè¯·è¿è¡Œ: `python scripts/generate_docs_index.py å¼€å‘æ–‡æ¡£`*
'''

        return content

    def save_index(self):
        """ä¿å­˜ç´¢å¼•æ–‡ä»¶"""
        index_path = self.docs_dir / 'KNOWLEDGE_INDEX.md'
        content = self.generate_index_content()

        with open(index_path, 'w', encoding='utf-8') as f:
            f.write(content)

        return index_path

def main():
    import sys

    if len(sys.argv) < 2:
        print("ç”¨æ³•: python generate_docs_index.py <å¼€å‘æ–‡æ¡£ç›®å½•>")
        print("ç¤ºä¾‹: python generate_docs_index.py å¼€å‘æ–‡æ¡£")
        sys.exit(1)

    docs_dir = sys.argv[1]

    if not os.path.exists(docs_dir):
        print(f"âŒ æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨: {docs_dir}")
        sys.exit(1)

    print(f"ğŸš€ å¼€å§‹ç”ŸæˆçŸ¥è¯†åº“ç´¢å¼•")
    print(f"ğŸ“ æ–‡æ¡£ç›®å½•: {docs_dir}\n")

    # åˆ›å»ºç´¢å¼•å™¨
    indexer = DocsIndexer(docs_dir)

    # æ‰«æå„ç±»æ–‡æ¡£
    print("ğŸ” æ‰«æåŠŸèƒ½æ¨¡å—æ–‡æ¡£...")
    indexer.scan_features()
    print(f"  âœ“ æ‰¾åˆ° {indexer.stats['features']} ä¸ªåŠŸèƒ½æ¨¡å—")

    print("ğŸ” æ‰«ææŠ€æœ¯å†³ç­–æ–‡æ¡£...")
    indexer.scan_decisions()
    print(f"  âœ“ æ‰¾åˆ° {indexer.stats['decisions']} ä¸ªADRæ–‡æ¡£")

    print("ğŸ” æ‰«ææŠ€æœ¯è®¾è®¡æ–‡æ¡£...")
    indexer.scan_technical()
    print(f"  âœ“ æ‰¾åˆ° {indexer.stats['technical_docs']} ä¸ªæŠ€æœ¯æ–‡æ¡£")

    print("ğŸ” æ‰«æé‡æ„æ–‡æ¡£...")
    indexer.scan_refactoring()
    print(f"  âœ“ æ‰¾åˆ° {indexer.stats['refactoring_docs']} ä¸ªé‡æ„æ–‡æ¡£")

    print("ğŸ” ç»Ÿè®¡å½’æ¡£æ–‡æ¡£...")
    indexer.count_archived()
    print(f"  âœ“ æ‰¾åˆ° {indexer.stats['archived_docs']} ä¸ªå½’æ¡£æ–‡æ¡£")

    # ç”Ÿæˆç´¢å¼•
    print("\nğŸ’¾ ç”ŸæˆçŸ¥è¯†åº“ç´¢å¼•...")
    index_path = indexer.save_index()
    print(f"  âœ… ç´¢å¼•ä¿å­˜è‡³: {index_path}")

    # è¾“å‡ºç»Ÿè®¡
    print(f"\nğŸ“Š çŸ¥è¯†åº“ç»Ÿè®¡:")
    print(f"  - åŠŸèƒ½æ¨¡å—: {indexer.stats['features']}")
    print(f"  - æŠ€æœ¯å†³ç­–: {indexer.stats['decisions']}")
    print(f"  - æŠ€æœ¯è®¾è®¡: {indexer.stats['technical_docs']}")
    print(f"  - é‡æ„æ–‡æ¡£: {indexer.stats['refactoring_docs']}")
    print(f"  - å†å²å½’æ¡£: {indexer.stats['archived_docs']}")
    print(f"  - æ´»è·ƒæ–‡æ¡£: {indexer.stats['total_files']}")
    print(f"  - æœ€è¿‘æ›´æ–°: {len(indexer.index['recent_updates'])} ä¸ªï¼ˆ30å¤©å†…ï¼‰")

    print(f"\nâœ¨ ç´¢å¼•ç”Ÿæˆå®Œæˆ!")
    print(f"ğŸ“– æŸ¥çœ‹ç´¢å¼•: {index_path}")

    return 0

if __name__ == '__main__':
    import sys
    sys.exit(main())
